{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff269e6",
   "metadata": {},
   "source": [
    "# Step 1 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac499c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_imdb_data(data_dir=\"/kaggle/input/imdbv1/aclImdb\", is_train=True):\n",
    "    \"\"\"读取IMDb训练集，返回(文本内容, 情感标签)列表\"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise FileNotFoundError(f\"目录不存在: {data_dir}\")\n",
    "    if is_train:\n",
    "        data_dir = os.path.join(data_dir, 'train')\n",
    "    else:\n",
    "        data_dir = os.path.join(data_dir, 'test')\n",
    "        \n",
    "    data = []\n",
    "    # 遍历正负样本文件夹\n",
    "    # neg: 0, pos: 1\n",
    "    for label, folder in enumerate([\"neg\", \"pos\"]):\n",
    "        folder_path = os.path.join(data_dir, folder)\n",
    "        # 匹配所有txt文件（排除隐藏文件）\n",
    "        file_pattern = os.path.join(folder_path, \"*.txt\")\n",
    "        for file_path in glob.glob(file_pattern):\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    text = f.read().strip()\n",
    "                    data.append( (text, label) )\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"跳过损坏文件: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"读取错误 {file_path}: {str(e)}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6342c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../imdb_aclImdb_v1/aclImdb\"\n",
    "# 读取训练集和测试集数据\n",
    "train_corpus = load_imdb_data(data_dir=data_dir, is_train=True)\n",
    "test_corpus = load_imdb_data(data_dir=data_dir, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23ba67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\",\n",
       " 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ff7ca4",
   "metadata": {},
   "source": [
    "# Step 2 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba124b85",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28cbb474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n",
      "Collecting nltk\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "     ------------- -------------------------- 0.5/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.0/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.3/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (6.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/38/ec/ad2d7de49a600cdb8dd78434a1aeffe28b9d6fc42eb36afab4a27ad23384/regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: colorama in d:\\nur\\developers\\anaconda_envs\\envs\\nlplab1\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5273549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    tokenized_corpus = []\n",
    "    for text, label in corpus:\n",
    "        tokens = word_tokenize(text)\n",
    "        tokenized_corpus.append((tokens, label))\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "190edd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_corpus = tokenize_corpus(train_corpus)\n",
    "tokenized_test_corpus = tokenize_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "013af85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Story',\n",
       "  'of',\n",
       "  'a',\n",
       "  'man',\n",
       "  'who',\n",
       "  'has',\n",
       "  'unnatural',\n",
       "  'feelings',\n",
       "  'for',\n",
       "  'a',\n",
       "  'pig',\n",
       "  '.',\n",
       "  'Starts',\n",
       "  'out',\n",
       "  'with',\n",
       "  'a',\n",
       "  'opening',\n",
       "  'scene',\n",
       "  'that',\n",
       "  'is',\n",
       "  'a',\n",
       "  'terrific',\n",
       "  'example',\n",
       "  'of',\n",
       "  'absurd',\n",
       "  'comedy',\n",
       "  '.',\n",
       "  'A',\n",
       "  'formal',\n",
       "  'orchestra',\n",
       "  'audience',\n",
       "  'is',\n",
       "  'turned',\n",
       "  'into',\n",
       "  'an',\n",
       "  'insane',\n",
       "  ',',\n",
       "  'violent',\n",
       "  'mob',\n",
       "  'by',\n",
       "  'the',\n",
       "  'crazy',\n",
       "  'chantings',\n",
       "  'of',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'singers',\n",
       "  '.',\n",
       "  'Unfortunately',\n",
       "  'it',\n",
       "  'stays',\n",
       "  'absurd',\n",
       "  'the',\n",
       "  'WHOLE',\n",
       "  'time',\n",
       "  'with',\n",
       "  'no',\n",
       "  'general',\n",
       "  'narrative',\n",
       "  'eventually',\n",
       "  'making',\n",
       "  'it',\n",
       "  'just',\n",
       "  'too',\n",
       "  'off',\n",
       "  'putting',\n",
       "  '.',\n",
       "  'Even',\n",
       "  'those',\n",
       "  'from',\n",
       "  'the',\n",
       "  'era',\n",
       "  'should',\n",
       "  'be',\n",
       "  'turned',\n",
       "  'off',\n",
       "  '.',\n",
       "  'The',\n",
       "  'cryptic',\n",
       "  'dialogue',\n",
       "  'would',\n",
       "  'make',\n",
       "  'Shakespeare',\n",
       "  'seem',\n",
       "  'easy',\n",
       "  'to',\n",
       "  'a',\n",
       "  'third',\n",
       "  'grader',\n",
       "  '.',\n",
       "  'On',\n",
       "  'a',\n",
       "  'technical',\n",
       "  'level',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'better',\n",
       "  'than',\n",
       "  'you',\n",
       "  'might',\n",
       "  'think',\n",
       "  'with',\n",
       "  'some',\n",
       "  'good',\n",
       "  'cinematography',\n",
       "  'by',\n",
       "  'future',\n",
       "  'great',\n",
       "  'Vilmos',\n",
       "  'Zsigmond',\n",
       "  '.',\n",
       "  'Future',\n",
       "  'stars',\n",
       "  'Sally',\n",
       "  'Kirkland',\n",
       "  'and',\n",
       "  'Frederic',\n",
       "  'Forrest',\n",
       "  'can',\n",
       "  'be',\n",
       "  'seen',\n",
       "  'briefly',\n",
       "  '.'],\n",
       " 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b435474",
   "metadata": {},
   "source": [
    "## build word dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51ec4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tokenized_corpus, min_freq=5):\n",
    "    \"\"\"\n",
    "    该函数用于根据分词后的语料库构建词表\n",
    "    :param tokenized_corpus: 分词后的语料库，格式为 [(tokens, label), ...]\n",
    "    :param min_freq: 词的最小出现频率，低于该频率的词将被过滤，默认为 5\n",
    "    :return: 最终词表、词到索引的映射、索引到词的映射\n",
    "    \"\"\"\n",
    "    all_tokens = []\n",
    "    for tokens, _ in tokenized_corpus:\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    # 统计词频\n",
    "    word_freq = Counter(all_tokens)\n",
    "\n",
    "    # 构建词表\n",
    "    vocab = sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "\n",
    "    # 过滤低频词\n",
    "    filtered_vocab = [word for word in vocab if word_freq[word] >= min_freq]\n",
    "\n",
    "    # 为词表添加特殊标记\n",
    "    special_tokens = ['<PAD>', '<UNK>']\n",
    "    final_vocab = special_tokens + filtered_vocab\n",
    "\n",
    "    # 创建词到索引的映射\n",
    "    word2idx = {word: idx for idx, word in enumerate(final_vocab)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(final_vocab)}\n",
    "\n",
    "    return final_vocab, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09809f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab, word2idx, idx2word = build_vocab(tokenized_train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb8a8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35813"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cab2c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20817, 1, 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['77'], word2idx['<UNK>'], word2idx['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb268b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'we')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[2], idx2word[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9fc53",
   "metadata": {},
   "source": [
    "## mapping word to idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de745793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(tokenized_corpus, word2idx, unk_token=\"<UNK>\"):\n",
    "    \"\"\"\n",
    "    将分词语料转换为ID序列（含UNK处理）\n",
    "    :param tokenized_corpus: 分词语料 [(tokens_list, label), ...]\n",
    "    :param word2idx: 词表映射 {word: index}\n",
    "    :param unk_token: 未登录词标记，默认\"<UNK>\"\n",
    "    :return: (id_sequences, labels) 元组\n",
    "    \"\"\"\n",
    "    unk_id = word2idx.get(unk_token, 0)  # 默认使用0作为UNK，需确保词表包含\n",
    "    id_sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for tokens, label in tokenized_corpus:\n",
    "        # 转换单个样本：token → id，UNK处理\n",
    "        seq_ids = [word2idx.get(token, unk_id) for token in tokens]\n",
    "        \n",
    "        # 简单校验（非空序列）\n",
    "        if not seq_ids:\n",
    "            print(f\"警告：空序列，标签{label}，原始tokens：{tokens}\")\n",
    "            continue\n",
    "        \n",
    "        id_sequences.append(seq_ids)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return id_sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6180e2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ID序列示例：\n",
      "\n",
      "样本1 - 标签：0\n",
      "分词长度：123 → ID序列：[1485, 7, 6, 171, 47, 56, 8727, 1410, 25, 6]...（前10个ID）\n",
      "对应分词：['Story', 'of', 'a', 'man', 'who', 'has', 'unnatural', 'feelings', 'for', 'a']...（前10个词）\n",
      "ID映射示例：Story → 1485\n",
      "\n",
      "样本2 - 标签：0\n",
      "分词长度：899 → ID序列：[9990, 21910, 535, 22, 6, 4418, 244, 9364, 14294, 1751]...（前10个ID）\n",
      "对应分词：['Airport', \"'77\", 'starts', 'as', 'a', 'brand', 'new', 'luxury', '747', 'plane']...（前10个词）\n",
      "ID映射示例：Airport → 9990\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 转换训练集和测试集\n",
    "train_ids, train_labels = text_to_ids(tokenized_train_corpus, word2idx)\n",
    "test_ids, test_labels = text_to_ids(tokenized_test_corpus, word2idx)\n",
    "\n",
    "# 结果预览（打印前2条）\n",
    "print(\"\\nID序列示例：\")\n",
    "for i in range(2):\n",
    "    print(f\"\\n样本{i+1} - 标签：{train_labels[i]}\")\n",
    "    print(f\"分词长度：{len(train_ids[i])} → ID序列：{train_ids[i][:10]}...（前10个ID）\")\n",
    "    print(f\"对应分词：{tokenized_train_corpus[i][0][:10]}...（前10个词）\")\n",
    "    print(f\"ID映射示例：{tokenized_train_corpus[i][0][0]} → {train_ids[i][0]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac89ec1",
   "metadata": {},
   "source": [
    "## mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92877bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ 1. 定义PyTorch Dataset ------------\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, id_sequences, labels, pad_id=0):\n",
    "        \"\"\"\n",
    "        :param id_sequences: train_ids/test_ids (列表的列表)\n",
    "        :param labels: rain_labels/test_labels (列表)\n",
    "        :param pad_id: <PAD>的ID (与词表一致, 默认0)\n",
    "        \"\"\"\n",
    "        assert len(id_sequences) == len(labels), \"数据与标签数量不匹配\"\n",
    "        self.seqs = id_sequences\n",
    "        self.labels = labels\n",
    "        self.pad_id = pad_id  # word2idx[<PAD>] = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx], self.labels[idx]  # 返回原始未填充的序列和标签\n",
    "\n",
    "# ------------ 2. 动态填充的Collate函数（关键！） ------------\n",
    "def imdb_collate(batch, pad_id=0):\n",
    "    \"\"\"\n",
    "    动态填充批次, 返回适合RNN的(填充序列, 长度, 标签)\n",
    "    :param batch: 原始批次数据 [(seq1, label1), (seq2, label2), ...]\n",
    "    :return: (padded_seqs, seq_lengths, labels)\n",
    "    \"\"\"\n",
    "    seqs, labels = zip(*batch)  # 解包批次\n",
    "    \n",
    "    # 计算每个序列的长度（用于RNN）\n",
    "    seq_lengths = torch.LongTensor([len(seq) for seq in seqs])\n",
    "    \n",
    "    # 动态填充到批次最大长度\n",
    "    max_len = seq_lengths.max().item()\n",
    "    padded_seqs = []\n",
    "    for seq in seqs:\n",
    "        # 填充方式：后补pad_id（与你的text_to_ids逻辑一致）\n",
    "        padded = seq + [pad_id] * (max_len - len(seq))\n",
    "        padded_seqs.append(torch.LongTensor(padded))  # 转换为LongTensor\n",
    "    \n",
    "    # 堆叠为批次张量\n",
    "    padded_seqs = torch.stack(padded_seqs)  # [B, L]\n",
    "    labels = torch.LongTensor(labels)  # [B]\n",
    "    \n",
    "    return padded_seqs, seq_lengths, labels  # 直接用于RNN的pack操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5771a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ 3. 创建DataLoader（使用你的数据） ------------\n",
    "# 假设你的词表中<PAD>的ID是0（在build_vocab中添加的第一个特殊标记）\n",
    "assert word2idx.get(\"<PAD>\", -1) == 0, \"请确保词表中<PAD>的ID是0\"\n",
    "\n",
    "# 训练集批次\n",
    "train_dataset = IMDBDataset(train_ids, train_labels, pad_id=0)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=imdb_collate,\n",
    "    num_workers=1,  # Kaggle建议2-4，根据内核调整\n",
    "    pin_memory=True  # GPU加速\n",
    ")\n",
    "\n",
    "# 测试集批次\n",
    "test_dataset = IMDBDataset(test_ids, test_labels, pad_id=0)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    collate_fn=imdb_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30851842",
   "metadata": {},
   "source": [
    "# Step 3 构建网络与训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59079c0e",
   "metadata": {},
   "source": [
    "## textcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf1fe94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_filters=64):\n",
    "        super().__init__()\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embed_dim, padding_idx=0\n",
    "        )\n",
    "        # 多尺度卷积核\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, embed_dim)) \n",
    "            for k in [3, 4, 5]  # 感受野3-5词\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * 3, 1)  # 3种核拼接\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, L] 填充后的ID序列（无需长度信息）\n",
    "        x = self.embedding(x)  # [B, L, E]\n",
    "        x = x.unsqueeze(1)     # [B, 1, L, E] 适配Conv2d\n",
    "        # 多尺度卷积+池化\n",
    "        conv_outs = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = conv(x).relu()  # [B, F, L-k+1, 1]\n",
    "            pooled = conv_out.squeeze(3).max(dim=2)[0]  # [B, F]\n",
    "            conv_outs.append(pooled)\n",
    "        # 拼接并分类\n",
    "        x = torch.cat(conv_outs, dim=1)  # [B, 3*F]\n",
    "        return self.fc(x).squeeze()  # [B]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892ce37",
   "metadata": {},
   "source": [
    "## bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6473fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, hidden_dim, \n",
    "            bidirectional=True, batch_first=True\n",
    "        )\n",
    "        # 注意力机制\n",
    "        self.attn = nn.Linear(hidden_dim*2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        # x: [B, L], lengths: [B]\n",
    "        x = self.embedding(x)  # [B, L, E]\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        outputs, (hn, cn) = self.lstm(packed)  # outputs: [B, L, 2H]\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        # 注意力加权\n",
    "        attn_scores = self.attn(outputs).squeeze(2)  # [B, L]\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        weighted = torch.bmm(outputs.permute(0, 2, 1), attn_weights.unsqueeze(2)).squeeze()  # [B, 2H]\n",
    "        \n",
    "        return self.fc(weighted).squeeze()  # [B]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5f7f6",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "557c9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化（使用你的词表长度)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TextCNN(vocab_size=len(final_vocab)).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二分类\n",
    "lr = 2e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d75175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/391 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# 训练循环（直接train_loader）\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()  # 记录开始时间\n",
    "    \n",
    "    # 使用 tqdm 包装 train_loader\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as t:\n",
    "        for batch_seqs, _, batch_labels in t:\n",
    "            batch_seqs = batch_seqs.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "            \n",
    "            outputs = model(batch_seqs)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * batch_seqs.size(0)\n",
    "            \n",
    "            # 更新 tqdm 描述信息\n",
    "            t.set_postfix(loss=loss.item())\n",
    "    \n",
    "    epoch_loss = total_loss / len(train_dataset)\n",
    "    elapsed_time = time.time() - start_time  # 计算耗时\n",
    "    print(f\"Epoch {epoch+1} | Loss: {epoch_loss:.4f} | Time: {elapsed_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629ac49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpLab1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
